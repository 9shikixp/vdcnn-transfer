{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:16.130728Z",
     "start_time": "2018-01-12T02:15:16.080594Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:17.108939Z",
     "start_time": "2018-01-12T02:15:16.567346Z"
    }
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import function, initializers, serializers, cuda\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:17.194806Z",
     "start_time": "2018-01-12T02:15:17.111491Z"
    }
   },
   "outputs": [],
   "source": [
    "class KMaxPooling1D(function.Function):\n",
    "    \n",
    "    def __init__(self, ndim, k):\n",
    "        if ndim <= 0:\n",
    "            raise ValueError(\n",
    "                'pooling operation requires at least one spatial dimension.')\n",
    "\n",
    "        self.ndim = ndim\n",
    "        self.k = k\n",
    "\n",
    "        self._used_cudnn = False\n",
    "\n",
    "    def forward_gpu(self, x):\n",
    "        if chainer.should_use_cudnn('>=auto') and 2 <= self.ndim <= 3:\n",
    "            # With cuDNN v3 or greater, use cuDNN implementation for inputs\n",
    "            # with spatial dimensions of two or more.\n",
    "            return super(KMaxPooling1D, self).forward_gpu(x)\n",
    "\n",
    "        self.retain_inputs(())\n",
    "        self._in_shape = x[0].shape\n",
    "        self._in_dtype = x[0].dtype\n",
    "\n",
    "        n, c = x[0].shape[:2]\n",
    "        dims = x[0].shape[2:]\n",
    "        ys = (self.k,)\n",
    "\n",
    "        y_shape = (n, c) + ys\n",
    "        y = cuda.cupy.empty(y_shape, dtype=x[0].dtype)\n",
    "        self.indexes = cuda.cupy.empty(y_shape, dtype=np.int32)\n",
    "        \n",
    "        cuda.elementwise('raw T in, int32 d_0, int32 out_0',\n",
    "                         'T out, S indexes', \n",
    "                         '''int c0 = i / (out_0);\n",
    "                            int out_x_0 = i % out_0;\n",
    "                            int in_x0_0 = 0;\n",
    "                            int in_x1_0 = d_0;\n",
    "                            int argmax_0[''' + str(self.k) + '''];\n",
    "                            T maxval[''' + str(self.k) + '''];\n",
    "                            for (int a = 0; a < out_0; ++a) {\n",
    "                              maxval[a] = (T)-(1.0/0.0);\n",
    "                              argmax_0[a] = -1;\n",
    "                            }\n",
    "                            for (int a = 0; a < out_0; ++a) {\n",
    "                              for (int x_0 = in_x0_0; x_0 < in_x1_0; ++x_0) {\n",
    "                                int offset_0 = 1 * (x_0 + d_0 * c0);\n",
    "                                int found = 0;\n",
    "                                for (int b = 0; b < a; ++b) {\n",
    "                                  if (argmax_0[b] == x_0) {\n",
    "                                    found = 1;\n",
    "                                    break;\n",
    "                                  }\n",
    "                                }\n",
    "                                if (found) {\n",
    "                                  continue;\n",
    "                                }\n",
    "                                T v = in[offset_0];\n",
    "                                if (maxval[a] < v) {\n",
    "                                  maxval[a] = v;\n",
    "                                  argmax_0[a] = x_0;\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            for (int a = 0; a < out_0; ++a) {\n",
    "                              for (int b = a + 1; b < out_0; ++b) {\n",
    "                                if (argmax_0[a] > argmax_0[b]) {\n",
    "                                  T tmpval = maxval[a];\n",
    "                                  int tmpindex = argmax_0[a];\n",
    "                                  maxval[a] = maxval[b];\n",
    "                                  argmax_0[a] = argmax_0[b];\n",
    "                                  maxval[b] = tmpval;\n",
    "                                  argmax_0[b] = tmpindex;\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            out = maxval[i % out_0];\n",
    "                            int argmax_k_0 = argmax_0[i % out_0];\n",
    "                            indexes = argmax_k_0;\n",
    "                         ''',\n",
    "                         'k_max_pool_1d_fwd')(\n",
    "            x[0].reduced_view(),\n",
    "            *(dims + ys +\n",
    "              (y, self.indexes)))\n",
    "                \n",
    "        return y,\n",
    "\n",
    "\n",
    "    def backward_gpu(self, x, gy):\n",
    "        if self._used_cudnn:\n",
    "            return super(KMaxPooling1D, self).backward_gpu(x, gy)\n",
    "\n",
    "        n, c = self._in_shape[:2]\n",
    "        dims = self._in_shape[2:]\n",
    "        ys = gy[0].shape[2:]\n",
    "        gx = cuda.cupy.empty(self._in_shape, self._in_dtype)\n",
    "\n",
    "        ndim = self.ndim\n",
    "        cuda.elementwise('raw T gy, raw S indexes, int32 d_0, int32 out_0',\n",
    "                         'T gx',\n",
    "                         '''operation:\n",
    "                            int c0  = i / (d_0);\n",
    "                            int x_0 = i % d_0;\n",
    "                            int out_x0_0 = 0;\n",
    "                            int out_x1_0 = out_0;\n",
    "                            T val = 0;\n",
    "                            for (int out_x_0 = out_x0_0; out_x_0 < out_x1_0; ++out_x_0) {\n",
    "                              int offset_0 = 1 * (out_x_0 + out_0 * c0);\n",
    "                              int kx = x_0;\n",
    "                              if (indexes[offset_0] == kx) {\n",
    "                                val = val + gy[offset_0];\n",
    "                              }\n",
    "                            }\n",
    "                            gx = val;\n",
    "                         ''',\n",
    "                         'k_max_pool_1d_bwd')(\n",
    "            gy[0].reduced_view(), self.indexes.reduced_view(),\n",
    "            *(dims + ys + (gx,)))\n",
    "        \n",
    "        return gx,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:17.697715Z",
     "start_time": "2018-01-12T02:15:17.694099Z"
    }
   },
   "outputs": [],
   "source": [
    "def k_max_pooling_1d(x, k):\n",
    "    ndim = len(x.shape[2:])\n",
    "    return KMaxPooling1D(ndim, k)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:18.096471Z",
     "start_time": "2018-01-12T02:15:18.082282Z"
    }
   },
   "outputs": [],
   "source": [
    "class BottleNeckA(chainer.Chain):\n",
    "\n",
    "    def __init__(self, in_size, ch_size):\n",
    "        initialW = initializers.HeNormal()\n",
    "        super(BottleNeckA, self).__init__(\n",
    "            conv1 = L.ConvolutionND(\n",
    "                1, in_size, ch_size, 3, 1, 1, initialW=initialW),\n",
    "            bn1 = L.BatchNormalization(ch_size),\n",
    "            conv2 = L.ConvolutionND(\n",
    "                1, ch_size, ch_size, 3, 1, 1, initialW=initialW),\n",
    "            bn2 = L.BatchNormalization(ch_size),\n",
    "            conv3 = L.ConvolutionND(\n",
    "                1, in_size, ch_size, 1, 1, 0,\n",
    "                initialW=initialW, nobias=True),\n",
    "            bn3 = L.BatchNormalization(ch_size)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        h1 = self.bn2(self.conv2(h1))\n",
    "        h2 = self.bn3(self.conv3(x))\n",
    "\n",
    "        return F.relu(h1 + h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:18.681226Z",
     "start_time": "2018-01-12T02:15:18.670794Z"
    }
   },
   "outputs": [],
   "source": [
    "class BottleNeckB(chainer.Chain):\n",
    "\n",
    "    def __init__(self, ch_size):\n",
    "        initialW = initializers.HeNormal()\n",
    "        super(BottleNeckB, self).__init__(\n",
    "            conv1 = L.ConvolutionND(\n",
    "                1, ch_size, ch_size, 3, 1, 1, initialW=initialW),\n",
    "            bn1 = L.BatchNormalization(ch_size),\n",
    "            conv2 = L.ConvolutionND(\n",
    "                1, ch_size, ch_size, 3, 1, 1, initialW=initialW),\n",
    "            bn2 = L.BatchNormalization(ch_size)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn1(self.conv1(x)))\n",
    "        h = self.bn2(self.conv2(h))\n",
    "\n",
    "        return F.relu(h + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:19.260917Z",
     "start_time": "2018-01-12T02:15:19.250008Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(chainer.ChainList):\n",
    "    \n",
    "    def __init__(self, layer, in_size, ch_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        if in_size != ch_size:\n",
    "            self.add_link(BottleNeckA(in_size, ch_size))\n",
    "        else:\n",
    "            self.add_link(BottleNeckB(ch_size))\n",
    "\n",
    "        for i in range(layer - 1):\n",
    "            self.add_link(BottleNeckB(ch_size))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:20.452201Z",
     "start_time": "2018-01-12T02:15:20.411023Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainedVDCNN(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_out):\n",
    "        super(TrainedVDCNN, self).__init__(\n",
    "            embed1 = L.EmbedID(3017, 50, initialW=initializers.HeUniform(), ignore_label=-1),\n",
    "            conv1 = L.ConvolutionND(1, 50, 64, 3, 1, 1, initialW=initializers.HeNormal()),\n",
    "            res2 = ConvBlock(2, 64, 64),\n",
    "            res3 = ConvBlock(2, 64, 128),\n",
    "            res4 = ConvBlock(2, 128, 256),\n",
    "            res5 = ConvBlock(2, 256, 512),\n",
    "            fc6 = L.Linear(4096, 2048),\n",
    "            fc7 = L.Linear(2048, 2048),\n",
    "            fc8 = L.Linear(2048, n_out)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = self.embed1(x)\n",
    "        h = h.transpose(0,2,1)\n",
    "        h = self.conv1(h)\n",
    "        h = self.res2(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res3(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res4(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res5(h)\n",
    "        h = k_max_pooling_1d(h, 8)\n",
    "        h = F.relu(self.fc6(h))\n",
    "        h = F.relu(self.fc7(h))\n",
    "        h = self.fc8(h)\n",
    "        \n",
    "        if t is not None:\n",
    "            loss = F.softmax_cross_entropy(h, t)\n",
    "            chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)\n",
    "            return loss\n",
    "        else:\n",
    "            return to_cpu(F.softmax(h).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:21.360565Z",
     "start_time": "2018-01-12T02:15:21.319907Z"
    }
   },
   "outputs": [],
   "source": [
    "class VDCNN(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, n_out):\n",
    "        super(VDCNN, self).__init__(\n",
    "            embed1 = L.EmbedID(3017, 50, initialW=initializers.HeUniform(), ignore_label=-1),\n",
    "            conv1 = L.ConvolutionND(1, 50, 64, 3, 1, 1, initialW=initializers.HeNormal()),\n",
    "            res2 = ConvBlock(2, 64, 64),\n",
    "            res3 = ConvBlock(2, 64, 128),\n",
    "            res4 = ConvBlock(2, 128, 256),\n",
    "            res5 = ConvBlock(2, 256, 512),\n",
    "            my_fc6 = L.Linear(4096, 2048),\n",
    "            my_fc7 = L.Linear(2048, 2048),\n",
    "            my_fc8 = L.Linear(2048, n_out)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        h = self.embed1(x)\n",
    "        h = h.transpose(0,2,1)\n",
    "        h = self.conv1(h)\n",
    "        h = self.res2(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res3(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res4(h)\n",
    "        h = F.max_pooling_nd(h, 3, 2, 1, cover_all=False)\n",
    "        h = self.res5(h)\n",
    "        h = k_max_pooling_1d(h, 8)\n",
    "        h = F.relu(self.my_fc6(h))\n",
    "        h = F.relu(self.my_fc7(h))\n",
    "        h = self.my_fc8(h)\n",
    "        \n",
    "        if t is not None:\n",
    "            loss = F.softmax_cross_entropy(h, t)\n",
    "            chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)\n",
    "            return loss\n",
    "        else:\n",
    "            return to_cpu(F.softmax(h).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:22.115921Z",
     "start_time": "2018-01-12T02:15:22.106768Z"
    }
   },
   "outputs": [],
   "source": [
    "['dokujo-tsushin',\n",
    " 'it-life-hack',\n",
    " 'kaden-channel',\n",
    " 'livedoor-homme',\n",
    " 'movie-enter',\n",
    " 'peachy',\n",
    " 'smax',\n",
    " 'sports-watch',\n",
    " 'topic-news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:15:46.004119Z",
     "start_time": "2018-01-12T02:15:45.999266Z"
    }
   },
   "outputs": [],
   "source": [
    "use_label = [1,3,7,8]\n",
    "label_change = {label:i for i, label in enumerate(use_label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:16:04.489588Z",
     "start_time": "2018-01-12T02:16:03.919099Z"
    }
   },
   "outputs": [],
   "source": [
    "_labels = []\n",
    "_sentences = []\n",
    "text_file = open('./data/livedoor-id.txt')\n",
    "line = text_file.readline()\n",
    "while line:\n",
    "    label, sentence = line[0], line.split(' ')[1:-1]\n",
    "    if int(label) in use_label:\n",
    "        id_sentence = np.ones(1024, dtype=np.float32) * (-1)\n",
    "        for i, c in enumerate(sentence):\n",
    "            id_sentence[i] = c\n",
    "        _labels.append(label_change[int(label)])\n",
    "        _sentences.append(id_sentence)\n",
    "    line = text_file.readline()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:16:05.267118Z",
     "start_time": "2018-01-12T02:16:05.259553Z"
    }
   },
   "outputs": [],
   "source": [
    "_sentences = np.array(_sentences, dtype=np.int32)\n",
    "_labels = np.array(_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:16:06.259770Z",
     "start_time": "2018-01-12T02:16:06.255596Z"
    }
   },
   "outputs": [],
   "source": [
    "from chainer.datasets import TupleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:16:06.873217Z",
     "start_time": "2018-01-12T02:16:06.868341Z"
    }
   },
   "outputs": [],
   "source": [
    "_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:16:09.332901Z",
     "start_time": "2018-01-12T02:16:09.328604Z"
    }
   },
   "outputs": [],
   "source": [
    "# is_test = np.arange(len(_labels)) % 10 == 0\n",
    "\n",
    "# train = TupleDataset(_sentences[~is_test], _labels[~is_test])\n",
    "\n",
    "# test = TupleDataset(_sentences[is_test], _labels[is_test])\n",
    "\n",
    "# del _sentences\n",
    "# del _labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:02.740541Z",
     "start_time": "2018-01-12T02:17:02.728227Z"
    }
   },
   "outputs": [],
   "source": [
    "is_test = np.arange(len(_labels)) % 10 == 0\n",
    "train_sentences = _sentences[~is_test]\n",
    "train_labels = _labels[~is_test]\n",
    "\n",
    "is_train = np.arange(len(train_labels)) % 16 == 0\n",
    "# is_train = np.arange(len(train_labels)) % 8 == 0\n",
    "# is_train = np.arange(len(train_labels)) % 4 == 0\n",
    "# is_train = np.arange(len(train_labels)) % 2 == 0\n",
    "\n",
    "train_sentences = train_sentences[is_train]\n",
    "train_labels = train_labels[is_train]\n",
    "\n",
    "train = TupleDataset(train_sentences, train_labels)\n",
    "\n",
    "test = TupleDataset(_sentences[is_test], _labels[is_test])\n",
    "\n",
    "del _sentences\n",
    "del _labels\n",
    "del train_sentences\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:07.494322Z",
     "start_time": "2018-01-12T02:17:07.489602Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:22.843209Z",
     "start_time": "2018-01-12T02:17:22.243298Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VDCNN(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:23.759648Z",
     "start_time": "2018-01-12T02:17:23.160601Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_model = TrainedVDCNN(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:24.088810Z",
     "start_time": "2018-01-12T02:17:23.760565Z"
    }
   },
   "outputs": [],
   "source": [
    "serializers.load_npz('./models/afpbb-epoch30.npz', trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:24.107426Z",
     "start_time": "2018-01-12T02:17:24.089770Z"
    }
   },
   "outputs": [],
   "source": [
    "def copy_model(src, dst):\n",
    "    assert isinstance(src, chainer.Chain)\n",
    "    assert isinstance(src, chainer.Chain)\n",
    "    for child in src.children():\n",
    "        if child.name not in dst.__dict__: continue\n",
    "        dst_child = dst[child.name]\n",
    "        if type(child) != type(dst_child): continue\n",
    "        if isinstance(child, chainer.Chain):\n",
    "            copy_model(child, dst_child)\n",
    "        if isinstance(child, chainer.Link):\n",
    "            match = True\n",
    "            for a, b in zip(child.namedparams(), dst_child.namedparams()):\n",
    "                if a[0] != b[0]:\n",
    "                    match = False\n",
    "                    break\n",
    "                if a[1].data.shape != b[1].data.shape:\n",
    "                    match = False\n",
    "                    break\n",
    "                if not match:\n",
    "                    print('Ignore %s because of parameter mismatch' % child.name)\n",
    "                    continue\n",
    "                for a, b in zip (child.namedparams(), dst_child.namedparams()):\n",
    "                    b[1].data = a[1].data\n",
    "                print('Copy %s' % child.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:25.937909Z",
     "start_time": "2018-01-12T02:17:25.925120Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "copy_model(trained_model, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:28.668470Z",
     "start_time": "2018-01-12T02:17:28.660256Z"
    }
   },
   "outputs": [],
   "source": [
    "class DelGradient(object):\n",
    "    name = 'DelGradient'\n",
    "    def __init__(self, delTgt):\n",
    "        self.delTgt = delTgt\n",
    "\n",
    "    def __call__(self, opt):\n",
    "        for name,param in opt.target.namedparams():\n",
    "            for d in self.delTgt:\n",
    "                if d in name:\n",
    "                    grad = param.grad\n",
    "                    with cuda.get_device(grad):\n",
    "                        grad *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-12T02:17:28.890986Z",
     "start_time": "2018-01-12T02:17:28.886273Z"
    }
   },
   "outputs": [],
   "source": [
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T05:22:01.126080Z",
     "start_time": "2018-01-11T05:21:09.438005Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter = chainer.iterators.MultiprocessIterator(train, batch_size=50)\n",
    "test_iter = chainer.iterators.MultiprocessIterator(test, batch_size=50, repeat=False)\n",
    "\n",
    "optimizer = chainer.optimizers.MomentumSGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "optimizer.add_hook(DelGradient([\"embed1\", \"conv1\", \"res2\", \"res3\", \"res4\", \"res5\"]))\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(5.0))\n",
    "\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=0)\n",
    "\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-2749/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-1375/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-688/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-344/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-172/')\n",
    "\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-2749-fixed/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-1375-fixed/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-688-fixed/')\n",
    "# trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-344-fixed/')\n",
    "trainer = training.Trainer(updater, (30, 'epoch'), out='result/transfer-4category-172-fixed/')\n",
    "\n",
    "class TestModeEvaluator(extensions.Evaluator):\n",
    "    def evaluate(self):\n",
    "        model = self.get_target('main')\n",
    "        with chainer.using_config('train', False):\n",
    "            ret = super(TestModeEvaluator, self).evaluate()\n",
    "        return ret\n",
    "\n",
    "trainer.extend(TestModeEvaluator(test_iter, model, device=0), trigger=(1, 'epoch'))\n",
    "trainer.extend(extensions.dump_graph('main/loss'))\n",
    "\n",
    "trainer.extend(extensions.LogReport(trigger=(1, 'epoch')))\n",
    "\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/accuracy', 'validation/main/accuracy', 'elapsed_time']\n",
    "                                     ), trigger=(1, 'epoch'))\n",
    "\n",
    "trainer.extend(extensions.ProgressBar(update_interval=30))\n",
    "\n",
    "trainer.extend(extensions.ExponentialShift('lr', 0.5), trigger=(3, 'epoch'))\n",
    "\n",
    "print('running')\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
